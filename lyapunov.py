"""Defines the LyapunovProblem dataclass â€” the common interface
that verifiers and the Lyapunov network are handed."""

from dataclasses import dataclass
from typing import Callable, List, Optional, Tuple
import numpy as np
import torch
from torch import nn, Tensor


@dataclass
class LyapunovProblem:
    """A NN Lyapunov function, dynamics, and region of interest.
    This is collectively what is verified.
    """

    nn_lyapunov: nn.Module
    dynamics: nn.Module
    region: Tensor

    f: List[Callable]
    domain: Tuple[Tuple[float, float], ...]
    x_star: np.ndarray
    dim: int
    analytic_V: Optional[Callable] = None
    analytic_V_gradient: Optional[Callable] = None
    zero_level_set_funcs: Optional[List[Callable]] = None


def lyapunov_loss_function(x_train_2d, practice_nn, f1_torch, f2_torch):
    """Composite Lyapunov training loss.

    Penalises:
      1. V(0) != 0  (origin condition)
      2. V(x) < 0   (positivity)
      3. V_dot >= 0  (Lie derivative must be strictly negative)
      4. Flat bowl   (prevents V from collapsing to a near-constant)
    """
    x_train_2d.requires_grad_(True)

    V_x = practice_nn(x_train_2d)

    # 1. Origin penalty
    origin = torch.zeros((1, 2), device=x_train_2d.device)
    V_0 = practice_nn(origin)
    origin_penalty = torch.pow(V_0, 2).mean()

    # 2. Positivity penalty
    positive_penalty = torch.relu(-V_x).mean()

    # 3. Lie derivative penalty
    grad_V = torch.autograd.grad(
        outputs=V_x,
        inputs=x_train_2d,
        grad_outputs=torch.ones_like(V_x),
        create_graph=True,
    )[0]
    f_vec = torch.cat([f1_torch(x_train_2d), f2_torch(x_train_2d)], dim=1)
    lie_derivative = torch.sum(grad_V * f_vec, dim=1)
    lie_penalty = torch.relu(lie_derivative + 1e-6).mean()

    # 4. Flatness penalty
    x_norm = torch.linalg.vector_norm(x_train_2d, dim=1, keepdim=True)
    flatness_penalty = torch.relu(x_norm - 0.1 * V_x).mean()

    return origin_penalty + positive_penalty + lie_penalty + flatness_penalty


def train_model_lyapunov_general(
    model, x_train_2d, num_epochs, learning_rate, f1_torch, f2_torch
):
    """Train a neural Lyapunov function using the composite Lyapunov loss."""
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        loss = lyapunov_loss_function(x_train_2d, model, f1_torch, f2_torch)
        loss.backward()
        optimizer.step()
        if (epoch + 1) % 100 == 0:
            print(f"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}")
    return model
